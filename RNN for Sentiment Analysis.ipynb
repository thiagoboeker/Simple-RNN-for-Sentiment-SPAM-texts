{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Category                                            Message\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "database = pd.read_csv('SPAM text message 20170820 - Data.csv')\n",
    "database.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validating the Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ham'\n",
      " 'Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...']\n"
     ]
    }
   ],
   "source": [
    "database = database.as_matrix()\n",
    "print(database[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ham\n"
     ]
    }
   ],
   "source": [
    "print(database[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n"
     ]
    }
   ],
   "source": [
    "print(database[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5572, 2)\n"
     ]
    }
   ],
   "source": [
    "print(database.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ndarray\n"
     ]
    }
   ],
   "source": [
    "print(type(database).__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = database[:, 1]\n",
    "labels = database[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ham\n"
     ]
    }
   ],
   "source": [
    "print(labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n"
     ]
    }
   ],
   "source": [
    "print(messages[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ndarray\n"
     ]
    }
   ],
   "source": [
    "print(type(messages).__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time to vetorize the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [''] #Because the 0 index will be use as padding\n",
    "for ii in range(messages.shape[0]):\n",
    "    for word in messages[ii].split():\n",
    "        vocab.append(word) #Create a list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86836\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15687\n"
     ]
    }
   ],
   "source": [
    "vocab_set = set(vocab) #Creates a set of unique words with indexes\n",
    "print(len(vocab_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create word2vec, vec2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = {} #Will be used to transform words in indexes\n",
    "vec2word = {} #Will be used to transform indexes in words\n",
    "\n",
    "for i, word in enumerate(vocab_set):\n",
    "    word2vec[word] = i\n",
    "    vec2word[i] = word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9603, 11709, 15406, 12286, 10448, 3541, 11690, 10338, 2993, 6028, 14043, 14207, 15470, 2584, 4395, 9572, 7786, 13634, 2773, 11147]\n"
     ]
    }
   ],
   "source": [
    "messages = np.array([[word2vec[word] for word in message.split()] for message in messages])\n",
    "print(messages[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "labels = np.array([1 if label == 'ham' else 0 for label in labels])\n",
    "print(labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "  9603. 11709. 15406. 12286. 10448.  3541. 11690. 10338.  2993.  6028.\n",
      " 14043. 14207. 15470.  2584.  4395.  9572.  7786. 13634.  2773. 11147.]\n"
     ]
    }
   ],
   "source": [
    "#Function to vetorize messages in fixed length arrays for the training\n",
    "\n",
    "def vectorizing_messages(messages):\n",
    "    \n",
    "    vector_message = np.zeros((messages.shape[0], 300))\n",
    "    \n",
    "    for i, row in enumerate(messages):\n",
    "        vector_message[i, -len(row):] = np.array(row)[:300]\n",
    "    \n",
    "    return vector_message\n",
    "\n",
    "messages = vectorizing_messages(messages)\n",
    "print(messages[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to split the database\n",
    "\n",
    "def set_training_data(messages, labels, train_size):\n",
    "    training_size = int(len(messages)*train_size)\n",
    "    train_x, val_x = messages[:training_size], messages[training_size:]\n",
    "    train_y, val_y = labels[:training_size], labels[training_size:]\n",
    "    \n",
    "    test_size = int(len(val_x)*0.5)\n",
    "    \n",
    "    val_x, test_x = val_x[:test_size], val_x[test_size:]\n",
    "    val_y, test_y = val_y[:test_size], val_y[test_size:]\n",
    "    \n",
    "    return train_x, val_x, train_y, val_y, test_x, test_y\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the placeholders for the data\n",
    "\n",
    "def get_inputs():\n",
    "    \n",
    "    X_ = tf.placeholder(tf.int32, shape=(None, None), name=\"inputs\")\n",
    "    y_ = tf.placeholder(tf.int32, shape=(None), name=\"labels\")\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    return X_, y_, keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Embedding the vocab_set, this will help to speed training because will embed words with his ouwn neighbours instead of passing\n",
    "#One big one-hot-encoded vector with all the words in the vocab_set\n",
    "\n",
    "def get_embed(X, embed_size, vocab_set):\n",
    "    \n",
    "    init_embeds = tf.Variable(tf.random_uniform((len(vocab_set), embed_size),-1, 1))\n",
    "    embeddings = tf.nn.embedding_lookup(init_embeds, X)\n",
    "    \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Return a Cell\n",
    "def RNN_Cell(lstm_size):\n",
    "    return tf.contrib.rnn.BasicLSTMCell(lstm_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build our RNN\n",
    "def build_RNN(lstm_size, embed, keep_prob, batch_size, num_layers):\n",
    "    \n",
    "    Cell = [RNN_Cell(lstm_size) for layer in range(num_layers)]\n",
    "    drop = [tf.contrib.rnn.DropoutWrapper(cell, input_keep_prob = keep_prob) for cell in Cell]\n",
    "    multi_rnn = tf.contrib.rnn.MultiRNNCell(drop)\n",
    "    initial_state = multi_rnn.zero_state(batch_size = batch_size, dtype=tf.float32)\n",
    "    initial_state = tf.identity(initial_state, \"initial_state\")\n",
    "    output, states = tf.nn.dynamic_rnn(multi_rnn, embed, dtype=tf.float32)\n",
    "\n",
    "    return initial_state, output, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Our output layer will transform the output of the RNN in a Logit, the RNN output comes in shape [batch_size, seq_length, lstm_size]\n",
    "#So we chose the last item in the seq_length column with all the batches and project it with a dense layer\n",
    "#This projection will pass to a sigmoid function to scale the output in a logit 0 to 1\n",
    "def output_layer(output, lstm_size,n_classes):\n",
    "    \n",
    "    logit = tf.layers.dense(output[:,-1], n_classes)\n",
    "    logits = tf.nn.sigmoid(logit)\n",
    "    \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HyperParameters\n",
    "lstm_size = 50\n",
    "learning_rate = 0.1\n",
    "batch_size = 100\n",
    "n_epochs = 5\n",
    "num_layers = 1\n",
    "embed_size = 200\n",
    "n_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seting the data\n",
    "train_x, val_x, train_y, val_y, test_x, test_y = set_training_data(messages, labels, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0          :           Cost:0.3357914984226227      :     Accuracy:0.9676840305328369\n",
      "Epoch:1          :           Cost:0.3400900065898895      :     Accuracy:0.9712746739387512\n",
      "Epoch:2          :           Cost:0.31378594040870667      :     Accuracy:0.9676840305328369\n",
      "Epoch:3          :           Cost:0.31363633275032043      :     Accuracy:0.9533213376998901\n",
      "Epoch:4          :           Cost:0.3153209686279297      :     Accuracy:0.9551166892051697\n"
     ]
    }
   ],
   "source": [
    "#Building training\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X, y, keep_prob = get_inputs() #placeholders\n",
    "\n",
    "embed = get_embed(X, embed_size, vocab_set) #embedding the placeholder\n",
    "\n",
    "initial_state, output, states = build_RNN(lstm_size, embed, keep_prob, batch_size, num_layers) #getting the RNN output\n",
    "\n",
    "logits = output_layer(output, lstm_size, n_classes) #getting the Logit\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = logits)) #Cost Function\n",
    "#Our cost function is a sparse_softmax, this helps in a way that we dont have to one-hot-encode tha labels\n",
    "\n",
    "optmizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "#Our optmizer will reduce the cost\n",
    "\n",
    "correct = tf.nn.in_top_k(logits, y, 1) #Get the correct predictions, we use tf.nn.in_top_k with sparse_softmax\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, \"float\")) #Calculate the mean of the right predictions\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    init.run()\n",
    "    initial_state.eval()\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        n_batches = int(len(train_x)/batch_size)\n",
    "        X_batches = np.array_split(train_x, n_batches)\n",
    "        y_batches = np.array_split(train_y, n_batches)\n",
    "        \n",
    "        for ii in range(n_batches):\n",
    "            \n",
    "            X_batch, y_batch = X_batches[ii], y_batches[ii]\n",
    "            \n",
    "            feed = {X: X_batch, y: y_batch, keep_prob:0.8}\n",
    "            \n",
    "            loss = cost.eval(feed)\n",
    "            opt = sess.run(optmizer, feed_dict = feed)\n",
    "            \n",
    "    \n",
    "        print(\"Epoch:{}          :           Cost:{}      :     Accuracy:{}\".format(epoch, loss, accuracy.eval({X:val_x, y:val_y, keep_prob:1})))\n",
    "        \n",
    "    save = saver.save(sess, './logs/rnn.ckpt')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing in the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./logs/rnn.ckpt\n",
      "Accuracy on test set:0.983739837398374\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, './logs/rnn.ckpt')\n",
    "    initial_state.eval()\n",
    "    pred = logits.eval({X:test_x, keep_prob:1})\n",
    "    results = np.argmax(pred, 1)\n",
    "    f1 = f1_score(results, test_y) \n",
    "    print('Accuracy on test set:{}'.format(f1))\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was some fun with RNN's, the code is very simplistic but yet we got a good result. I choose'd to not tokenize pontuaction on the messages such as \"!\" or \"?\" so I know i could improve on that, but the overall results was satisfacting for me.\n",
    "Thanks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
